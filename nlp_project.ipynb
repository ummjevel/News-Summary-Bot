{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1t4yVUau5HvvEF9-OvpNQw6O8iNHLRWIB","authorship_tag":"ABX9TyMGjE+f37JGaztfonw1veTv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"e03959c8d7a44cf4a9852adeb61acf55":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ac1814848e04f6a93beef3202580438","IPY_MODEL_2891a1664077429f84945a5bee8eebbf","IPY_MODEL_dfaade3cf1464759b5869c008b9829f6"],"layout":"IPY_MODEL_0598f9bec3524b7b9122fcd5d42a1162"}},"0ac1814848e04f6a93beef3202580438":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b064393f67f24ddf8dbf97a6d0c11fea","placeholder":"​","style":"IPY_MODEL_33ccb780e925460b9d6a017385408cbb","value":"Downloading: 100%"}},"2891a1664077429f84945a5bee8eebbf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d171c5653a146bda0671cf0710c4784","max":2329707353,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e803c596199d4753b8e83b36d781ac01","value":2329707353}},"dfaade3cf1464759b5869c008b9829f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1ede3020d31478fbb4e885a72c3149a","placeholder":"​","style":"IPY_MODEL_098350ca90904edbb9d641818b443be8","value":" 2.33G/2.33G [00:49&lt;00:00, 48.5MB/s]"}},"0598f9bec3524b7b9122fcd5d42a1162":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b064393f67f24ddf8dbf97a6d0c11fea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33ccb780e925460b9d6a017385408cbb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d171c5653a146bda0671cf0710c4784":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e803c596199d4753b8e83b36d781ac01":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f1ede3020d31478fbb4e885a72c3149a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"098350ca90904edbb9d641818b443be8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a0afb0058ab493bbd0cbddb556647d4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1695bb7ed7624836845d298dada93e9f","IPY_MODEL_0b206f2a656844939326cd4149f51640","IPY_MODEL_889ae8b8fd00431c90fea65818f1e4e3"],"layout":"IPY_MODEL_a41f38d18c334870a070bf50f671073e"}},"1695bb7ed7624836845d298dada93e9f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c715482ecd2d4e40a3bc5ef94a1d8bdb","placeholder":"​","style":"IPY_MODEL_3852724bac314870b44d0e0d95cb7a2a","value":"Downloading: 100%"}},"0b206f2a656844939326cd4149f51640":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eafe8bd9861649a2ad7742d74549369f","max":2329707353,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7e6e018438b14b6ba5fc2c68eca90697","value":2329707353}},"889ae8b8fd00431c90fea65818f1e4e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a2434dddb5842f680375d926acf7057","placeholder":"​","style":"IPY_MODEL_fa5dc16704274089a31d61490795014a","value":" 2.33G/2.33G [00:42&lt;00:00, 30.2MB/s]"}},"a41f38d18c334870a070bf50f671073e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c715482ecd2d4e40a3bc5ef94a1d8bdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3852724bac314870b44d0e0d95cb7a2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eafe8bd9861649a2ad7742d74549369f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e6e018438b14b6ba5fc2c68eca90697":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a2434dddb5842f680375d926acf7057":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa5dc16704274089a31d61490795014a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# install transformers"],"metadata":{"id":"kbSoQg8N7wN6"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tzScaogh5OqW","executionInfo":{"status":"ok","timestamp":1672358013897,"user_tz":-540,"elapsed":11138,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}},"outputId":"756f9897-bd3f-4368-f058-ca48a3fd4c57"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 74.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 66.3 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"]}]},{"cell_type":"markdown","source":["# example"],"metadata":{"id":"1unK_-Lx7uAS"}},{"cell_type":"code","source":["from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"ainize/klue-bert-base-mrc\")\n","model = AutoModelForQuestionAnswering.from_pretrained(\"ainize/klue-bert-base-mrc\")\n","\n","context = \"명탐정 코난은 1996년에 방영이 시작된 애니메이션으로 고등학생 명탐정 쿠도 신이치가 검은 조직의 거래현장을 목격하다가 습격당하여 어린아이로 변한 후의 이야기이다. 전형적인 탐정물이며, 에도가와 코난이란 이름으로 활동하게 되는데, 이런 명대사가 있다. '에도가와 코난, 탐정이지.' 코난이 등장하는 곳은 살인사건이 일어나는데 시청자로써는 끝나지 않는 애니메이션이라 좋은 점도 있지만 많은 사람들이 죽어나가는 것을 보면 조금 안타깝기도 하다.\"\n","question = \"요약하면?\"\n","\n","encodings = tokenizer(context, question, max_length=512, truncation=True,\n","                      padding=\"max_length\", return_token_type_ids=False)\n","encodings = {key: torch.tensor([val]) for key, val in encodings.items()}             \n","\n","input_ids = encodings[\"input_ids\"]\n","attention_mask = encodings[\"attention_mask\"]\n","\n","pred = model(input_ids, attention_mask=attention_mask)\n","\n","start_logits, end_logits = pred.start_logits, pred.end_logits\n","\n","token_start_index, token_end_index = start_logits.argmax(dim=-1), end_logits.argmax(dim=-1)\n","\n","pred_ids = input_ids[0][token_start_index: token_end_index + 1]\n","\n","prediction = tokenizer.decode(pred_ids)\n","prediction"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"NXaY8s0v6gTM","executionInfo":{"status":"ok","timestamp":1672234232483,"user_tz":-540,"elapsed":5928,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}},"outputId":"4094cc25-581d-4b78-860d-45c4f5539594"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"에도가와 코난, 탐정이지.'코난이 등장하는 곳은 살인사건\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w6N8L2oPEGrL","executionInfo":{"status":"ok","timestamp":1672358018143,"user_tz":-540,"elapsed":4247,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}},"outputId":"a9ae44a8-da13-427a-d4cd-be447a4e138e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 5.2 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n"]}]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P75AQ9_HFHPN","executionInfo":{"status":"ok","timestamp":1672358024449,"user_tz":-540,"elapsed":6309,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}},"outputId":"26c46bd5-94b9-4a51-bb81-3f7115adf589"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n","\u001b[K     |████████████████████████████████| 452 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n","Collecting xxhash\n","  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n","\u001b[K     |████████████████████████████████| 213 kB 65.1 MB/s \n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 68.0 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 73.7 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.8.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.2.0\n"]}]},{"cell_type":"code","source":["import re\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n","\n","article_text = \"명탐정 코난은 1996년에 방영이 시작된 애니메이션으로 고등학생 명탐정 쿠도 신이치가 검은 조직의 거래현장을 목격하다가 습격당하여 어린아이로 변한 후의 이야기이다. 전형적인 탐정물이며, 에도가와 코난이란 이름으로 활동하게 되는데, 이런 명대사가 있다. '에도가와 코난, 탐정이지.' 코난이 등장하는 곳은 살인사건이 일어나는데 시청자로써는 끝나지 않는 애니메이션이라 좋은 점도 있지만 많은 사람들이 죽어나가는 것을 보면 조금 안타깝기도 하다.\"\n","\n","model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","\n","input_ids = tokenizer(\n","    [WHITESPACE_HANDLER(article_text)],\n","    return_tensors=\"pt\",\n","    padding=\"max_length\",\n","    truncation=True,\n","    max_length=512\n",")[\"input_ids\"]\n","\n","output_ids = model.generate(\n","    input_ids=input_ids,\n","    max_length=84,\n","    no_repeat_ngram_size=2,\n","    num_beams=4\n",")[0]\n","\n","summary = tokenizer.decode(\n","    output_ids,\n","    skip_special_tokens=True,\n","    clean_up_tokenization_spaces=False\n",")\n","\n","print(summary)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123,"referenced_widgets":["e03959c8d7a44cf4a9852adeb61acf55","0ac1814848e04f6a93beef3202580438","2891a1664077429f84945a5bee8eebbf","dfaade3cf1464759b5869c008b9829f6","0598f9bec3524b7b9122fcd5d42a1162","b064393f67f24ddf8dbf97a6d0c11fea","33ccb780e925460b9d6a017385408cbb","2d171c5653a146bda0671cf0710c4784","e803c596199d4753b8e83b36d781ac01","f1ede3020d31478fbb4e885a72c3149a","098350ca90904edbb9d641818b443be8"]},"id":"kpjz6Z3lDSyU","executionInfo":{"status":"ok","timestamp":1672236649768,"user_tz":-540,"elapsed":83100,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}},"outputId":"2209ebac-1c94-4530-b30d-b18f6d4e2f0b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e03959c8d7a44cf4a9852adeb61acf55"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["에도가와 코난이 등장하는 탐정물.\n"]}]},{"cell_type":"markdown","source":["# load data"],"metadata":{"id":"R16quFM557m7"}},{"cell_type":"code","source":["train_file_path = '/content/drive/MyDrive/Alpaco_NLP/training/tl_news.pkl'\n","test_file_path = '/content/drive/MyDrive/Alpaco_NLP/validation/vl_news.pkl'\n","\n","import pickle\n","\n","with open(train_file_path, 'rb') as f:\n","    df_train = pickle.load(f)\n","\n","with open(test_file_path, 'rb') as f:\n","    df_test = pickle.load(f)"],"metadata":{"id":"do_0iUKF9dMQ","executionInfo":{"status":"ok","timestamp":1672269835310,"user_tz":-540,"elapsed":4173,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["X_train_list = df_train['original'].tolist()\n","X_test_list = df_test['original'].tolist()\n","y_train = df_train['summary'].tolist()\n","y_test = df_test['summary'].tolist()"],"metadata":{"id":"1dbE4m9F95ck","executionInfo":{"status":"ok","timestamp":1672269835311,"user_tz":-540,"elapsed":2,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# save data to tsv file\n","import csv\n","from tqdm import tqdm\n","\n","with open('/content/drive/MyDrive/Alpaco_NLP/train.tsv', 'w', encoding='utf-8', newline='') as f:\n","    tw = csv.DictWriter(f, fieldnames = [\"news\", \"summary\"], delimiter='\\t')\n","    tw.writeheader()\n","    for original, summary in zip(tqdm(df_train['original']), df_train['summary']):\n","        tw.writerow({'news': original.replace('\\n', ' '), 'summary': summary.replace('\\n', ' ')})\n","with open('/content/drive/MyDrive/Alpaco_NLP/test.tsv', 'w', encoding='utf-8', newline='') as f:\n","    tw = csv.DictWriter(f, fieldnames = [\"news\", \"summary\"], delimiter='\\t')\n","    tw.writeheader()\n","    for original, summary in zip(tqdm(df_test['original']), df_test['summary']):\n","        tw.writerow({'news': original.replace('\\n', ' '), 'summary': summary.replace('\\n', ' ')})\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gyb1fIHxGW2x","executionInfo":{"status":"ok","timestamp":1672270709936,"user_tz":-540,"elapsed":2301,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}},"outputId":"528d7671-e779-44f9-c1fe-30452e9474ca"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["\n","  0%|          | 0/21600 [00:00<?, ?it/s]\u001b[A\n","  7%|▋         | 1424/21600 [00:00<00:01, 14236.01it/s]\u001b[A\n"," 13%|█▎        | 2848/21600 [00:00<00:01, 13937.27it/s]\u001b[A\n"," 20%|█▉        | 4243/21600 [00:00<00:01, 13286.67it/s]\u001b[A\n"," 26%|██▌       | 5576/21600 [00:00<00:01, 13300.78it/s]\u001b[A\n"," 32%|███▏      | 6943/21600 [00:00<00:01, 13426.93it/s]\u001b[A\n"," 38%|███▊      | 8303/21600 [00:00<00:00, 13453.02it/s]\u001b[A\n"," 45%|████▍     | 9650/21600 [00:00<00:00, 12820.43it/s]\u001b[A\n"," 51%|█████     | 10938/21600 [00:00<00:00, 12812.50it/s]\u001b[A\n"," 57%|█████▋    | 12224/21600 [00:00<00:00, 12671.68it/s]\u001b[A\n"," 62%|██████▏   | 13494/21600 [00:01<00:00, 12368.14it/s]\u001b[A\n"," 68%|██████▊   | 14734/21600 [00:01<00:00, 12363.51it/s]\u001b[A\n"," 74%|███████▍  | 15973/21600 [00:01<00:00, 11962.54it/s]\u001b[A\n"," 80%|███████▉  | 17266/21600 [00:01<00:00, 12241.51it/s]\u001b[A\n"," 86%|████████▌ | 18529/21600 [00:01<00:00, 12354.79it/s]\u001b[A\n"," 92%|█████████▏| 19899/21600 [00:01<00:00, 12749.94it/s]\u001b[A\n","100%|██████████| 21600/21600 [00:01<00:00, 12234.18it/s]\n","\n","  0%|          | 0/2700 [00:00<?, ?it/s]\u001b[A\n","100%|██████████| 2700/2700 [00:00<00:00, 13068.78it/s]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","df_train = pd.read_csv(\"/content/drive/MyDrive/Alpaco_NLP/train.tsv\", delimiter='\\t')\n","df_test = pd.read_csv(\"/content/drive/MyDrive/Alpaco_NLP/test.tsv\", delimiter='\\t')"],"metadata":{"id":"BMOb-1FQGg3W","executionInfo":{"status":"ok","timestamp":1672358027921,"user_tz":-540,"elapsed":3474,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# load model, tokenizer"],"metadata":{"id":"A_wHmawe9aU2"}},{"cell_type":"code","source":["from transformers import AutoModelForQuestionAnswering, AutoTokenizer, TFAutoModelForSeq2SeqLM, AutoModelForSeq2SeqLM\n","\n","model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["4a0afb0058ab493bbd0cbddb556647d4","1695bb7ed7624836845d298dada93e9f","0b206f2a656844939326cd4149f51640","889ae8b8fd00431c90fea65818f1e4e3","a41f38d18c334870a070bf50f671073e","c715482ecd2d4e40a3bc5ef94a1d8bdb","3852724bac314870b44d0e0d95cb7a2a","eafe8bd9861649a2ad7742d74549369f","7e6e018438b14b6ba5fc2c68eca90697","8a2434dddb5842f680375d926acf7057","fa5dc16704274089a31d61490795014a"]},"id":"rlCOIqgw9UKp","executionInfo":{"status":"ok","timestamp":1672316165635,"user_tz":-540,"elapsed":58429,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}},"outputId":"61c32929-72ea-4be3-aaae-90d0682d7b31"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a0afb0058ab493bbd0cbddb556647d4"}},"metadata":{}}]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":294},"id":"ov0NMRAKFz8f","executionInfo":{"status":"error","timestamp":1672316165636,"user_tz":-540,"elapsed":5,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}},"outputId":"77c84573-a553-46ed-e799-21205ced75f5"},"execution_count":6,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1266\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'MT5ForConditionalGeneration' object has no attribute 'summary'"]}]},{"cell_type":"code","source":["import pandas as pd\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","from transformers import AdamW, get_scheduler\n","from datasets import load_metric\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","from tqdm.auto import tqdm\n","\n","class SummaryDataset(Dataset):\n","    def __init__(\n","        self,\n","        data,\n","        tokenizer=tokenizer,\n","        text_max_token_len = 256,\n","        summary_max_token_len = 128\n","    ):\n","        self.tokenizer = tokenizer\n","        self.data = data\n","        self.text_max_token_len = text_max_token_len\n","        self.summary_max_token_len = summary_max_token_len\n","    \n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index: int):\n","        data_row = self.data.iloc[index]\n","\n","        text = data_row['news']\n","\n","        text_encoding = tokenizer(\n","            text,\n","            max_length=self.text_max_token_len,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            add_special_tokens=True,\n","            return_tensors='pt'\n","        )\n","\n","        summary_encoding = tokenizer(\n","            data_row['summary'],\n","            max_length=self.summary_max_token_len,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            add_special_tokens=True,\n","            return_tensors='pt'\n","        )\n","        \n","        labels = summary_encoding['input_ids']\n","        labels[labels == tokenizer.pad_token_id] = -100\n","        \n","        return dict(\n","            input_ids=text_encoding['input_ids'].flatten(),\n","            attention_mask=text_encoding['attention_mask'].flatten(),\n","            labels=labels.flatten(),\n","            decoder_attention_mask=summary_encoding['attention_mask'].flatten()\n","        )\n","\n","train_dataset = SummaryDataset(data=df_train)\n","test_dataset = SummaryDataset(data=df_test)\n","\n","train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2)\n","eval_dataloader = DataLoader(test_dataset, batch_size=2)"],"metadata":{"id":"sXu4AZXzELC7","executionInfo":{"status":"aborted","timestamp":1672316165636,"user_tz":-540,"elapsed":3,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# train\n","\n","https://www.kaggle.com/code/ibnmohey/mt5-multilingual-xlsum-cbd321"],"metadata":{"id":"ze6jH2tnZ2mS"}},{"cell_type":"code","source":["num_epochs = 1\n","\n","num_training_steps = num_epochs * len(train_dataloader)\n","\n","optimizer = AdamW(model.parameters())\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps\n",")\n","\n","progress_bar = tqdm(range(num_training_steps))\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","model.train()\n","for epoch in range(num_epochs):\n","    for batch in train_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        \n","        outputs = model(**batch)\n","#         logits = outputs.logits\n","#         predictions = torch.argmax(logits, dim=-1)\n","#         print(predictions)\n","#         print(batch[\"labels\"])\n","        \n","        loss = outputs.loss\n","        loss.backward()\n","        \n","        optimizer.step()\n","        lr_scheduler.step()\n","        \n","        optimizer.zero_grad()\n","        progress_bar.update()\n","    \n","    torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss,\n","            }, f'/content/drive/MyDrive/Alpaco_NLP/t5-kor.pth')\n","\n","    print(f'epoch: {epoch + 1} -- loss: {loss}')"],"metadata":{"id":"IADApKpG9XHh","executionInfo":{"status":"aborted","timestamp":1672315944195,"user_tz":-540,"elapsed":5,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","d = torch.load('/content/drive/MyDrive/Alpaco_NLP/t5-kor.pth')\n","d"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1UQxpQPH0DTfntJohkLNULpnQUtgRt9nL"},"id":"jF-ai0a5Ft1t","executionInfo":{"status":"ok","timestamp":1672316746342,"user_tz":-540,"elapsed":43121,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}},"outputId":"ac89a9cc-2364-4224-afe1-296f928e959d"},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["d.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0yTRpxK2jnz","executionInfo":{"status":"ok","timestamp":1672316864247,"user_tz":-540,"elapsed":542,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}},"outputId":"9f3065a1-a1a1-49c8-f5c6-0083eccd3f08"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss'])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["model.load_state_dict(d['model_state_dict'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h_86Wd2M0jf5","executionInfo":{"status":"ok","timestamp":1672317433637,"user_tz":-540,"elapsed":1085,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}},"outputId":"69b3947f-6ee4-42e5-b699-8041f02ce94c"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5arvEMHi4u3k","executionInfo":{"status":"ok","timestamp":1672317440556,"user_tz":-540,"elapsed":617,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}},"outputId":"e6ac9333-3f0b-4d7a-b843-5b64aa3df2d1"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MT5ForConditionalGeneration(\n","  (shared): Embedding(250112, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(250112, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(250112, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=250112, bias=False)\n",")"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["text = '''\n","노형욱 장관, 철도역 방역ㆍ운행 안전에 만전강조\\n 14일 서울역 KTX 철도역사열차 방역실태 전반에 걸쳐 현장 점검\\n 철도특별사법경찰대에는 방역수칙 위반자에 대한 무관용 원칙 대응 주문 \\n 노형욱 국토교통부 장관은 7월 14일 수도권 코로나19 방역의 관문이라고 할 수 있는 서울역을 방문하여, 철도역사 및 열차 방역실태 등을 점검하고, 관계자들을 격려하였다.\\n 먼저, 노 장관은 한국철도공사로부터 수도권 거리두기 4단계 격상에 따른 전국 주요 역사 탑승 전 발열체크 등 강화된 철도분야 방역대책을 보고 받은 뒤, 코로나19 로 어려운 상황 속에서도 국민을 위해 힘써 주시는 노고에 감사드린다면서 관계자들을 격려하였다.\\n 이어, KTX 방역현장을 둘러본 뒤, 지금까지 철도분야는 철두철미한 방역조치로 열차 내 감염사례가 단 한 건도 없었다라며, 강화된 방역대책이 승객에게 잘 안내되고, 이를 통해 실제 현장에서 잘 이행될 수 있도록 사명감을 갖고 총력을 기울여 줄 것을 강조하였다.\\n 또한 방역으로 인해 자칫 운행 안전에 대한 경각심이 느슨해질 수 있다면서, 촘촘하고 치밀한 시설물 점검 및 차량의 정비를 통해 안전사고 예방에도 만전을 기해줄 것을 당부하였다.\\n 이어, 서울지방철도특별사법경찰대를 방문한 노 장관은 철도시설 내 국민의 지킴이 역할과 해외입국자 KTX 전용칸 수송지원 업무를 차질 없이 수행하고 있는 철도경찰의 노고에 감사드린다면서,\\n 코로나19로 인해 철도 승객 간 분쟁이 증가하는 등 철도경찰의 역할이 그 어느 때보다 중요한 때라며, 승객의 방역수칙 위반에 대해서는 무관용 원칙을 적용하여 국민들이 안심하고 철도를 이용할 수 있는 환경을 조성해 줄 것을 당부하였다\n","'''\n","\n","max_input_length = 1024\n","max_target_length = 128\n","\n","model_inputs = tokenizer(\n","        text, max_length=max_input_length, truncation=True\n","    )\n","\n","predictions = model.generate(torch.tensor([model_inputs['input_ids']]), max_length = max_input_length)\n","\n","pred = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","pred"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YaEiUSlp4wiy","executionInfo":{"status":"ok","timestamp":1672317500624,"user_tz":-540,"elapsed":24411,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}},"outputId":"0d1c7cb4-d394-4f5a-e618-6c4658752f0e"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['노 국토교통부 장관은 철도 분야는 철두철미한 방역조치로 열차 내 감염사례가 단 한 건도 없었다고 말했다.']"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["import torch\n","\n","!pip install keras-nlp --upgrade\n","\n","!pip install rouge-score --upgrade\n","\n","from tensorflow import keras\n","import rouge_score\n","import keras_nlp\n","from keras_nlp.metrics.rouge_base import RougeBase\n","rouge_l = keras_nlp.metrics.RougeL()\n","y_true = \"노형욱 국토교통부 장관은 7월 14일 수도권 코로나19 방역의 관문이라고 할 수 있는 서울역을 방문하여, 철도역사 및 열차 방역실태 등을 점검하고, 관계자들을 격려하였다. 또한 방역으로 인해 자칫 운행 안전에 대한 경각심이 느슨해질 수 있다면서, 촘촘하고 치밀한 시설물 점검 및 차량의 정비를 통해 안전사고 예방에도 만전을 기해줄 것을 당부하였다.\"\n","y_pred = '노 국토교통부 장관은 철도 분야는 철두철미한 방역조치로 열차 내 감염사례가 단 한 건도 없었다고 말했다.'\n","rouge_l(y_true, y_pred)[\"f1_score\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I5X8Lrm943OH","executionInfo":{"status":"ok","timestamp":1672317700084,"user_tz":-540,"elapsed":135562,"user":{"displayName":"JMJ mseagle","userId":"05181043881737298106"}},"outputId":"f3a17787-d5e0-4de2-acd8-9bf77b010b31"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting keras-nlp\n","  Downloading keras_nlp-0.4.0-py3-none-any.whl (337 kB)\n","\u001b[K     |████████████████████████████████| 337 kB 19.8 MB/s \n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from keras-nlp) (1.3.0)\n","Collecting tensorflow-text\n","  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 60.8 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from keras-nlp) (21.3)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (from keras-nlp) (2.9.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from keras-nlp) (1.21.6)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->keras-nlp) (3.0.9)\n","Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (2.9.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (3.3.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (2.1.1)\n","Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (2.9.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (0.28.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (0.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (4.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (1.6.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (57.4.0)\n","Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (1.12)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (0.2.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (3.19.6)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (1.14.1)\n","Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (2.9.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (3.1.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (1.51.1)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (1.1.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (1.15.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-nlp) (14.0.6)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow->keras-nlp) (0.38.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (1.8.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (2.23.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (1.0.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (0.6.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (2.15.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (3.4.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (0.4.6)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (0.2.8)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (5.2.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (5.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (3.11.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras-nlp) (3.2.2)\n","Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-text->keras-nlp) (0.12.0)\n","Collecting tensorflow\n","  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n","\u001b[K     |████████████████████████████████| 588.3 MB 21 kB/s \n","\u001b[?25hINFO: pip is looking at multiple versions of tensorflow-text to determine which version is compatible with other requirements. This could take a while.\n","Collecting tensorflow-text\n","  Downloading tensorflow_text-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n","\u001b[K     |████████████████████████████████| 5.9 MB 61.6 MB/s \n","\u001b[?25hCollecting tensorflow\n","  Downloading tensorflow-2.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n","\u001b[K     |████████████████████████████████| 578.1 MB 8.0 kB/s \n","\u001b[?25h  Downloading tensorflow-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n","\u001b[K     |████████████████████████████████| 578.1 MB 6.8 kB/s \n","\u001b[?25hCollecting tensorflow-text\n","  Downloading tensorflow_text-2.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n","\u001b[K     |████████████████████████████████| 4.6 MB 64.2 MB/s \n","\u001b[?25hInstalling collected packages: tensorflow-text, keras-nlp\n","Successfully installed keras-nlp-0.4.0 tensorflow-text-2.9.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from rouge-score) (1.3.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from rouge-score) (3.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from rouge-score) (1.21.6)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from rouge-score) (1.15.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk->rouge-score) (4.64.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->rouge-score) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->rouge-score) (2022.6.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->rouge-score) (7.1.2)\n","Building wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=7558c73b8db8e7e44035635e3b102038bbd92bcf10db2b8094826b9ece76b385\n","  Stored in directory: /root/.cache/pip/wheels/24/55/6f/ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\n","Successfully built rouge-score\n","Installing collected packages: rouge-score\n","Successfully installed rouge-score-0.1.2\n"]},{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":[],"metadata":{"id":"MWmOGErJ5PO9"},"execution_count":null,"outputs":[]}]}